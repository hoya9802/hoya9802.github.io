---
layout: single
title:  "Paper Review - DenseNet(Densely Connected Convolutional Networks)"
typora-root-url: ../
categories: Paper-Review
tag: [CNN, iamge classification, DenseNet]
author_profile: false
sidebar:
    nav: 'counts'
search: true
use_math: true
redirect_from:
  - /paper/DenseNet
published: true
---

**[Reference]** [CVPR2017 DenseNet Paper](https://arxiv.org/pdf/1608.06993.pdf){:target="_blank"}
{: .notice}

## Abstract

해당 논문은 Dense Convolutional Network(DenseNet)을 적용하여 각 층을 이후 모든 레이어로 연결하는 방식으로 모든 다른 층에 연결하는 신경망입니다.

각각의 레이어의 모든 이전 레이어의 feature map들을 input으로 사용되어 지고, 또한 그렇게 만들어진 output feature map도 이후 모든 레이어의 input으로 사용함으로서 몇가지 강력한 이점을 가집니다.
 - Gradient vanishing issue를 완화시킬 수 있다.
 - 강력한 feature들을 전파시킨다.
 - Feature들을 재사용하도록 유도한다.
 - Model parameters을 상당히 줄일 수 있다.

## 1. Introduction

![DenseNet_Figure1](/images/2023-12-05-DenseNet/DenseNet_Figure1.png){: .img-width-half .align-center}

CNN 모델들의 신경망이 깊어짐에 따라 gradient가 모델의 끝까지 도달하지 못하여 Vanishing gradient 문제들이 발생하였습니다. 이 문제를 ResNet, Highway Network는 하나의 층에서 다음 층으로 identity connections을 이용하여 신호를 우회시켜 해당 문제를 해결하였습니다.

비록 서로 다른 접근법들이 network의 구조나 학습 절차을 변화시켰지만, 기본적으로 그들은 모두 이전 레이어의 값을 이후 레이어의 short path로 연결한다는 것은 일치하였습니다.

해당 논문에서는 maximum inforamtion을 네트워크의 모든 레이어 사이로 확실하게 흐르게 하기 위해서 feature map 사이즈을 일치시킨 상태로 모든 layers을 연결한다. 기존의 feed-forward을 유지하기 위해서 각각의 layer은 이전 모든 layers로부터 정보들을 얻고 자신의 정보또한 이후 모든 레이어들로 전달시킨다 (see Figure <span style='color:red'>1</span>).

ResNet과는 다르게 각각의 features을 <span style='color:red'>**summation하는 것이 아니라 concatenate를 진행한다.**</span> 만약 L개의 layer를 가지는 네트워크가 있다면 총 $\frac{L(L+1)}{2}$개의 connections을 가지게 된다.

이로인한 반직관적인 효과로는 전통적인 convolutional networks과 다르게 중복된 특징 맵을 다시 학습시킬 필요가 없기 때문에 더 적은 parameter가 필요하게 됩니다. 전통적인 순방향 architecture들의 각 layer들은 그 전 layer으로부터 통과를 한 후, 이를 다음 층으로 전달하게 됩니다. 이에 따라 각 layer들은 state을 변경하면서 보존되어야 할 정보를 전달합니다.

## 2. Related Work
