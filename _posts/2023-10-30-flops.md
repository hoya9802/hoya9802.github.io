---
layout: single
title:  "딥러닝 - FLOPs와 모델 최적화(Model Optimization)"
typora-root-url: ../
categories: Deep-Learning
tag: [Deep Learning, Math, Model Optimization]
author_profile: false
sidebar:
    nav: 'counts'
search: true
use_math: true
redirect_from:
  - /DL/FLOPs
published: false
---

## FLOPs란
FLOPs란 Floating Point Operations의 약자로, 초당 수행되는 부동 소수점 연산의 양을 나타내는 지표입니다. 쉽게 말하면 딥러닝에서 모델이 순전파(Forward-Propagation)를 진행할때 곱셈, 나눗셈, 덧셈, 뺄셈등을 얼마나 많이 사용하는지 나타내는 지표입니다. 따라서 우리는 모델의 FLOPs를 알 수 있다면 모델을 더 나은 성능을 발휘하고 추론하는 최적화된 모델을 설계할 수 있게 됩니다. 하지만 우리는 FLOPs를 측정하기 위해서는 FLOP, FMA, MACs를 알아야 합니다.

## FLOPS
FLOPS는 Floating Point Operations per Second의 약자이며 마지막 S가 대문자입니다. 그리고 이것은 하드웨어가 얼마나 좋은 성능을 가지고 있는지 알려주는 줍니다. 따라서 FLOPS가 크면 클수록 초당 하드웨어가 처리 할 수 있는 작업이 많다는 것이고, 이것은 추론 속도의 향상으로 이어지게 됩니다.

## FMA
FMA는 Floating point Multiply and Add operation의 약자로, 특히 그래픽 처리 장치(GPU)와 같은 하드웨어에서 고성능 연산을 수행하는 것으로, FMA는 A*x + B를 하나의 연산으로 처리합니다. 

## MACs
MAC는 Multiply-Accumulate Computations의 약자로, FMA가 모델에서 몇번 실행되었는지 계산하는 것이 바로 MAC입니다. 하지만 FLOPs는 FMA에 있는 2개의 연산(곱하기, 더하기)를 별개로 취급하기 때문에 결국 MAC : FLOPs = 1 : 2 비율을 가지게 됩니다. 따라서 FLOPs를 계산할때는 MACs의 곱하기 2를 해주면 됩니다.

## 모델의 FLOPs 계산방법
* 입력이미지는 28x28x1(Gray scale)입니다.
*

![flops_ex1.webp](/images/2023-11-1-FLOPs/flops_ex1.webp)





* Reference<br>
[https://www.thinkautonomous.ai/blog/deep-learning-optimization/#operations_nb](https://www.thinkautonomous.ai/blog/deep-learning-optimization/#operations_nb)
[https://www.youtube.com/watch?v=T7o3xvJLuHk&t=67s](https://www.youtube.com/watch?v=T7o3xvJLuHk&t=67s)